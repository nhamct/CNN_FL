{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import copy\n",
    "import sklearn\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "#import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior() \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import imblearn.over_sampling as smote\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from matplotlib.axis import Axis   \n",
    "import matplotlib.ticker as ticker \n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_coverage(coverage_file, spectra_file):\n",
    "    \n",
    "    #spectrum data and their labels. Main data for the project.\n",
    "    spectrum_data = list() \n",
    "    spectrum_labels = list()\n",
    "    \n",
    "    \"\"\" if(Path(spectra_file)).exists():\n",
    "        with open(spectra_file) as name_file:\n",
    "            element_names = {i: name.strip() for i, name in enumerate(name_file)}\n",
    "\n",
    "        n_elements = len(element_names) \"\"\"\n",
    "    \n",
    "    if Path(coverage_file).exists():\n",
    "        matrix_file = open(coverage_file)\n",
    "        for line in matrix_file.readlines():\n",
    "\n",
    "            line = line.split(\" \")\n",
    "            if line[-1]=='+\\n':\n",
    "                line[-1] = 0 #0 for passing test\n",
    "            else: # always '-'\n",
    "                line[-1] = 1 #1 for failing test\n",
    "                            \n",
    "            #y = line[-1]\n",
    "            #X = line[:-1]\n",
    "\n",
    "            y = float(line[-1])  # Convert y to float\n",
    "            X = [float(x) for x in line[:-1]]  # Convert X to float\n",
    "            \n",
    "            \"\"\"  if y==1:\n",
    "                for i in range(len(X)): #Filter down to union of components executed by faulty test cases.\n",
    "                    if X[i]==\"1\":\n",
    "                        important_column_numbers.add(i)  \"\"\"\n",
    "                        \n",
    "            spectrum_data.append(X)\n",
    "            spectrum_labels.append(y)\n",
    "\n",
    "    \"\"\"  spectrum_data_reduced = list()\n",
    "    important_column_numbers = list(important_column_numbers)\n",
    "    \n",
    "    for line in spectrum_data:\n",
    "        line = [int(i) for i in line]\n",
    "        spectrum_data_reduced.append([line[i] for i in important_column_numbers]) #NhamCT-chi loc nhung line co thuc hien TC. CHange to numpy\n",
    "    spectrum_data_reduced= np.array(spectrum_data_reduced) \"\"\"\n",
    "   \n",
    "    return spectrum_data, spectrum_labels#, important_column_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_shape, 1)),  \n",
    "    tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output là 1 nếu fail, 0 nếu pass\n",
    "])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "        optimizer=RMSprop(learning_rate=1e-4),\n",
    "        metrics=['accuracy'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROJECTS = ['Closure', 'Lang', 'Chart', 'Math', 'Mockito', 'Time']\n",
    "PROJECT_BUGS = [\n",
    "    [str(x) for x in range(1, 134)],\n",
    "    [str(x) for x in range(1, 66)],\n",
    "    [str(x) for x in range(1, 27)],\n",
    "    [str(x) for x in range(1, 107)],\n",
    "    [str(x) for x in range(1, 39)],\n",
    "    [str(x) for x in range(1, 28)]\n",
    "] \"\"\"\n",
    "\n",
    "PROJECTS = ['Chart']\n",
    "PROJECT_BUGS = [[str(x) for x in range(1, 9)]]\n",
    "\n",
    "input_data_dir=\"D:\\TEMP\\Matrix\"\n",
    "output_dir = \"D:\\TEMP\\Sus\"\n",
    "fig_dir = \"D:\\TEMP\\Fig\"\n",
    "model_dir = \"Models\"\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_files = sorted(glob(os.path.join(input_data_dir, '*coverage')))\n",
    "spectra_files = sorted(glob(os.path.join(input_data_dir, '*spectra')))\n",
    "accuracies = []\n",
    "for project, bugs in zip(PROJECTS, PROJECT_BUGS):\n",
    "    for bug in bugs:\n",
    "        coverage_file = os.path.join(input_data_dir,'%s-%s-coverage' %(project, bug))\n",
    "        spectra_file = os.path.join(input_data_dir,'%s-%s-spectra' %(project, bug))\n",
    "\n",
    "        spectrum_data, spectrum_labels = get_test_coverage(coverage_file, spectra_file)\n",
    "        spectrum_data = tf.convert_to_tensor(spectrum_data, dtype=None,dtype_hint=None, name=None)\n",
    "        spectrum_labels = tf.convert_to_tensor(spectrum_labels,dtype=None,dtype_hint=None, name=None)\n",
    "\n",
    "        with open(spectra_file) as name_file:\n",
    "            element_names = {i: name.strip() for i, name in enumerate(name_file)}\n",
    "        n_elements = len(element_names)\n",
    "        \n",
    "        #imbalance process\n",
    "        sm = smote.RandomOverSampler(random_state=42)\n",
    "        X, y  = sm.fit_resample(spectrum_data, spectrum_labels)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model = create_model(n_elements)\n",
    "        history = model.fit(X, y, epochs=EPOCHS, batch_size=1, shuffle=True, validation_split=0.2)\n",
    "        model.save(save_format='h5',filepath=os.path.join(model_dir,'%s-%s-cnn.h5'%(project, bug)))\n",
    "\n",
    "        accuracies.append((project, bug, history.history['accuracy'], history.history['val_accuracy']))\n",
    "\n",
    "        #Plot accuracy\n",
    "        train_accuracy = history.history['loss']\n",
    "        val_accuracy = history.history['val_loss']\n",
    "\n",
    "        plt.plot(train_accuracy, label='Training Loss')\n",
    "        plt.plot(val_accuracy, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('%s-%s' %(project, bug))\n",
    "        #plt.show()\n",
    "        plt.savefig(os.path.join(fig_dir,'%s-%s-cnn.jpg' % (project, bug)))\n",
    "        plt.close('all')\n",
    "\n",
    "        suspiciousness = []\n",
    "        virtual_test_set = [[1 if j==i else 0 for j in range(n_elements)] for i in range(n_elements)]\n",
    "        for i in range(n_elements):\n",
    "            virtual_feature = np.array(virtual_test_set[i]).reshape(1,-1)\n",
    "            output = model.predict(virtual_feature)\n",
    "            suspiciousness.append(output[0])\n",
    "        \n",
    "        with open(os.path.join(output_dir, '%s-%s-cnn-suspiciousness' % (project, bug)), 'w') as output_file:\n",
    "            writer = csv.DictWriter(output_file, ['Statement','Suspiciousness'])\n",
    "            writer.writeheader()\n",
    "            for element in range(n_elements):\n",
    "                writer.writerow({ 'Statement': element_names[element],'Suspiciousness': suspiciousness[element]})\n",
    "\n",
    "with open(os.path.join(output_dir, '%s-%s-cnn-accuracy' % (project, bug)), 'w', newline='') as accuary_file:\n",
    "    writer_acc = csv.writer(accuary_file)\n",
    "    for row in accuracies:\n",
    "        writer_acc.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
